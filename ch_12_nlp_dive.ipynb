{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWqHugxRTjWZfoiaqTo7kq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshgregory42/practical_deep_learning/blob/main/ch_12_nlp_dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Language Model from Scratch"
      ],
      "metadata": {
        "id": "i5f1XXBdX2lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Data\n",
        "\n",
        "Dataset is called *Human Numbers*, which contains the first 10,000 numbers written out in English. This is a dataset that will let us try out methods quickly and easily and interpret the results.\n",
        "\n",
        "Download the dataset the usual way:"
      ],
      "metadata": {
        "id": "C-evlE5JYR1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "zKtiZKLZXyHe",
        "outputId": "710e6c0d-891b-4054-f23c-87f54e237eed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='32768' class='' max='30252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      108.32% [32768/30252 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-pZLUOTYi--",
        "outputId": "c172ed8a-fb1e-4a1f-bf21-0a3d13df389f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('/root/.fastai/data/human_numbers/train.txt'),Path('/root/.fastai/data/human_numbers/valid.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open up those two files and see what's inside. First will join everything together and ignore the train/valid split (will come back to it later):"
      ],
      "metadata": {
        "id": "NWd5PDuQYlJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = L()\n",
        "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f: lines += L(*f.readlines())"
      ],
      "metadata": {
        "id": "spsYXjPKYjjL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take all those lines and concatenate them, separating them with '.':"
      ],
      "metadata": {
        "id": "sWhMMuUKY2k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' . '.join([l.strip() for l in lines])\n",
        "\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mz3N6CSUYzqH",
        "outputId": "99589bd9-4a2b-470d-d78d-c34cb543d923"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize by splitting on spaces:"
      ],
      "metadata": {
        "id": "1sku02EnZAZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split(' ')\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXT-iZgjY-3o",
        "outputId": "51eb1aba-d0ea-4549-a6e8-467d60e7c1da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To numericalize, have to create a list of all the unique tokens (our *vocab*):"
      ],
      "metadata": {
        "id": "-eK_39cwZFt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = L(*(tokens)).unique()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyIztk0uZEFn",
        "outputId": "a3918b7e-7c20-4ab1-8e2b-cdf69988e0c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert our tokens into numbers by looking up the index of each in the vocab:"
      ],
      "metadata": {
        "id": "2G008y3pZO3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w:i for i, w in enumerate(vocab)}\n",
        "\n",
        "nums = L(word2idx[i] for i in tokens)\n",
        "nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COmij2JLZMnm",
        "outputId": "9617bb4a-78f9-46f0-e351-aedd2dbe5da0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Lanuage Model from Scratch\n",
        "\n",
        "A simple way to turn this into a neural network would be to specify that we are going to predict each word based on the previous three words. Could create a list of every sequence of three words as our independent variables, and the next word after each sequence as the dependent variable.\n",
        "\n",
        "Can do that with plain Python. First do it with tokens to confirm what it looks like:"
      ],
      "metadata": {
        "id": "B22tKoa2aCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2axHMnl5ZYWE",
        "outputId": "717ab0f5-49ce-457d-e246-0c81666a9b02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now do it with tensors of the numericalized values, which is what the model will actually use:"
      ],
      "metadata": {
        "id": "GQBRxv95alSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))\n",
        "seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mShzcbgMaiQv",
        "outputId": "71cc7827-b2e6-4814-c345-f6b5207d16fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can batch those using the `DataLoader` class. For now we'll split the sequences randomly:"
      ],
      "metadata": {
        "id": "qqcqe1lEbhKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
      ],
      "metadata": {
        "id": "CioptDt1be3S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. Use three standard layers, with a few changes:\n",
        "\n",
        "First change is that the first linear layer will only use the first word's embedding as activations, the second layer will use the second word's embedding plus the first layer's output activations, and the third layer will use the third word's embedding plus the second layer's output activations. Key effect here is that every word is interpreted in the information context of any words preceding it.\n",
        "\n",
        "Second main change is that each of these three layers will use the same weight matrix. This means that the way one word impacts the activations from previous words should not change depending on the position of the word. So a layer does not learn one sequence position; must learn to handle all positions.\n",
        "\n",
        "Since layer weights don't change, could think of the sequential layers as \"the same layer\" repeated."
      ],
      "metadata": {
        "id": "Glgaepq7cBre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Language Model in PyTorch\n",
        "\n",
        "Create the language model module that we described earlier:"
      ],
      "metadata": {
        "id": "y09d22xmfsaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = F.relu(self.h_h(self.i_h(x[:, 0])))\n",
        "    h = h + self.i_h(x[:, 1])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    h = h + self.i_h(x[:, 2])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    return self.h_o(h)"
      ],
      "metadata": {
        "id": "ezezQ6lkbxii"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've created three layers here:\n",
        "\n",
        "* The embedding layer (`i_h`, for *input* to *hidden*)\n",
        "* The linear layer to create the activations for the next word (`h_h`, for *hidden* to *hidden*)\n",
        "* A final linear layer to predict the fourth word (`h_o`, for *hidden* to *output*)\n",
        "\n"
      ],
      "metadata": {
        "id": "3y3v88dhjLvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try training this model and see what happens:"
      ],
      "metadata": {
        "id": "WE4N8zltjn4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                              metrics=accuracy)\n",
        "\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "iBy24gN-ix4D",
        "outputId": "edc275d9-6e1b-4e62-d4d8-5240897c7a78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.831179</td>\n",
              "      <td>1.918370</td>\n",
              "      <td>0.470644</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.394328</td>\n",
              "      <td>1.802045</td>\n",
              "      <td>0.468267</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.410896</td>\n",
              "      <td>1.697776</td>\n",
              "      <td>0.491086</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.393379</td>\n",
              "      <td>1.648621</td>\n",
              "      <td>0.490849</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare this to what a really simple model would give us. We could always predict the mode common token, so let's find out which token is most often the target in our validation set:"
      ],
      "metadata": {
        "id": "-orVFypbkLL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n, counts = 0, torch.zeros(len(vocab))\n",
        "\n",
        "for x, y in dls.valid:\n",
        "  n += y.shape[0]\n",
        "  for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
        "\n",
        "idx = torch.argmax(counts)\n",
        "idx, vocab[idx.item()], counts[idx].item()/n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vy6-cXkS8G",
        "outputId": "057521c5-8e0a-46bd-90bc-06225f67e279"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most common token has the index of 29, which corresponds to `thousand`. So if we always predicted this token we would have an accuracy of roughly 15\\%, so our model is doing much better.\n",
        "\n",
        "This baseline is okay. Let's see how we can refactor it with a loop."
      ],
      "metadata": {
        "id": "1dAvFwufkm_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our First Recurrent Neural Network\n",
        "\n",
        "We could simplify our module code by replacing it with code that calls the layers with a `for` loop. This would make our code simpler and also let us apply our module equally well to token sequences of different lengths. Won't be limited to token lists of length three:"
      ],
      "metadata": {
        "id": "hFd0zLIClaYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel2(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = 0\n",
        "        for i in range(3):\n",
        "            h = h + self.i_h(x[:,i])\n",
        "            h = F.relu(self.h_h(h))\n",
        "        return self.h_o(h)"
      ],
      "metadata": {
        "id": "xvCIgQyYklfh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that we get the same results using this refactoring:"
      ],
      "metadata": {
        "id": "gUQgwysqlxGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "z6eQECXtluYx",
        "outputId": "28366ccd-3fac-4a23-8a17-f67b239683af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.845536</td>\n",
              "      <td>2.057746</td>\n",
              "      <td>0.478488</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.396628</td>\n",
              "      <td>1.765957</td>\n",
              "      <td>0.483480</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.397797</td>\n",
              "      <td>1.665134</td>\n",
              "      <td>0.493939</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.347163</td>\n",
              "      <td>1.651769</td>\n",
              "      <td>0.494176</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that a neural network that has a loop like this is called a *recurrent neural network* (RNN). An RNN isn't something new, it's just a refactoring of a multilayer neural network using a `for` loop. Could just call it a \"looping neural network\" and it would mean the same thing."
      ],
      "metadata": {
        "id": "UwFKu0IpmlAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the RNN\n",
        "\n"
      ],
      "metadata": {
        "id": "GbxIr7cjm0dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some issues from the code for our RNN:\n",
        "\n",
        "We are initializing our hidden state to zero for every new input sequence. This is a problem because we made our sample sequences short so they would fit easily into batches. But if we order the samples corectly, those sample sequences will be read in order by the model, exposing the model to long stretches of the original sequence.\n",
        "\n",
        "Can also look at having more signal. Why predict only the fourth word when we could use the intermediate predictions to also predict the second and third words?"
      ],
      "metadata": {
        "id": "XMo_En_zk5FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maintaining the State of an RNN\n",
        "\n",
        "Since we initialize the model's hidden state to zero for each new sample, we're throwing away all the info about the sentences we've already seen. The fix here is to put the initialization of the hidden state into `__init__`.\n",
        "\n",
        "But here's the catch: Makes out NN as deep as the number of tokens in our document. So if we have 10,000 tokens in our dataset, we would have a 10,000-layer neural network. Problem here is that if we get to the 10,000th word in the dataset, need to calculate all of those derivatives, which would be really slow and probably not work.\n",
        "\n",
        "Solution: Tell PyTorch that we don't want to backpropagate the derivatives through the entire implicit neural network. Instead we'll just keep the last three layers of gradient.\n",
        "\n",
        "To remove all of the gradient history in PyTorch, use the `detach` method.\n",
        "\n",
        "So here's the new RNN, which is not stateful:"
      ],
      "metadata": {
        "id": "cbWFsARBlTV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel3(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i in range(3):\n",
        "      self.h = self.h + self.i_h(x[:, i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "\n",
        "    out = self.h_o(self.h)\n",
        "    self.h = self.h.detach()\n",
        "    return out\n",
        "\n",
        "  def reset(self): self.h = 0\n"
      ],
      "metadata": {
        "id": "Db0uIQ-Il-P5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has the same activations whatever sequence length we pick, since the hidden state will remember the last activation from the previous batch.\n",
        "\n",
        "Difference here is that we'll use something called *backpropagation through time* (BPTT), where the gradients computed at each step will only be calculated on sequence length tokens in the past, **not** the whole stream.\n",
        "\n",
        "BPTT: Treating a neural net with effectively one layer per step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, usually use *truncated* BPTT, which \"detaches\" the history of computation steps in the hidden state every few time steps."
      ],
      "metadata": {
        "id": "DSu8ikc_JNeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into groups:"
      ],
      "metadata": {
        "id": "sP7MFq5_LWFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = len(seqs)//bs # // is floor division (divide then round down)\n",
        "\n",
        "m, bs, len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDBGzymKI92R",
        "outputId": "fd7ec897-b4b1-4e06-e750-23618ba0238e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328, 64, 21031)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First batch will be composed of the samples\n",
        "\n",
        "$$ (0, m, 2m, \\ldots, (bs-1) m) $$\n",
        "\n",
        "Second batch:\n",
        "\n",
        "$$ (1, m+1, 2m+1, \\ldots (bs-1)m + 1) $$\n",
        "\n",
        "and so on. So at each epoch, the model will see a chunk of contiguous text of size $3m$ (since each text is of size 3) on each line of the batch.\n",
        "\n",
        "Function that does that reindexing:"
      ],
      "metadata": {
        "id": "GRtb5snWLp3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_chunks(ds, bs):\n",
        "  m = len(ds) // bs\n",
        "  new_ds = L()\n",
        "  for i in range(m): new_ds += L(ds[i+m*j] for j in range(bs))\n",
        "  return new_ds"
      ],
      "metadata": {
        "id": "dFqfjui2LhCD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WT-qB2P4cKBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then pass `drop_last=True` when building our `DataLoaders` to drop the last batch that does not have the correct shape. Also pass `shuffle=False` so that the texts are read in order:"
      ],
      "metadata": {
        "id": "7-E0x2mDbzb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cut = int(len(seqs) * 0.8)\n",
        "\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs),\n",
        "    group_chunks(seqs[cut:], bs),\n",
        "    bs=bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "WDjRm9FMbnVc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also add a `Callback`."
      ],
      "metadata": {
        "id": "3q827u0lcNbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "\n",
        "learn.fit_one_cycle(10, 3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "g7MAHDtGcKfZ",
        "outputId": "d6a7f752-f1e6-497e-d36c-cfc66e48cccc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.717644</td>\n",
              "      <td>1.880074</td>\n",
              "      <td>0.436538</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.261081</td>\n",
              "      <td>1.610183</td>\n",
              "      <td>0.515385</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.077195</td>\n",
              "      <td>1.648596</td>\n",
              "      <td>0.521154</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.004429</td>\n",
              "      <td>1.610231</td>\n",
              "      <td>0.543269</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.957333</td>\n",
              "      <td>1.632679</td>\n",
              "      <td>0.567308</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.894831</td>\n",
              "      <td>1.969951</td>\n",
              "      <td>0.556731</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.873104</td>\n",
              "      <td>1.625450</td>\n",
              "      <td>0.581971</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.812236</td>\n",
              "      <td>1.727251</td>\n",
              "      <td>0.609615</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.773130</td>\n",
              "      <td>1.743647</td>\n",
              "      <td>0.622115</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.759798</td>\n",
              "      <td>1.742210</td>\n",
              "      <td>0.620913</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating More Signal\n",
        "\n",
        "An issue here is that we only predict one output word for every three input words. So the amount of signal that we're feeding back to update weights with is not as large as it could be. Would be better if we predicted the next word after every single word rather than every three words.\n",
        "\n",
        "To do this, we need to first change our data so that the dependent variable has each of the three next words after each of our three input words. Instead of `3`, we use the attribute `sl` (sequence length) and make it a bit bigger:"
      ],
      "metadata": {
        "id": "okjY0hTnlf0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sl = 16\n",
        "\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "          for i in range(0, len(nums)-sl-1, sl))\n",
        "\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
        "                             group_chunks(seqs[cut:], bs),\n",
        "                             bs=bs, drop_last=True, shuffle=False)"
      ],
      "metadata": {
        "id": "oJjb2iWkcawv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First element of `seqs` contains two lists of the same size. Second list is the same as the first, but offset by one element:"
      ],
      "metadata": {
        "id": "cb1YZ_Rh4vBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ],
      "metadata": {
        "id": "SAjCCcxP4uJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef1749e-a20a-4ee0-f710-05feb3f7138a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
              " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now modify our model so it outputs a prediction after every word, rather than just at the end of a three-word sequence:"
      ],
      "metadata": {
        "id": "Y4DukjS1DRI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel4(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    outs = []\n",
        "    for i in range(sl):\n",
        "      self.h = self.h + self.i_h(x[:, i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "      outs.append(self.h_o(self.h))\n",
        "    self.h = self.h.detach()\n",
        "    return torch.stack(outs, dim=1)\n",
        "\n",
        "  def reset(self): self.h = 0"
      ],
      "metadata": {
        "id": "w8gJJyMUDMHY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returns outputs of shape `bs x sl x vocab_sz` (since we stacked on `dim=1`). Targets are of shape `bs x sl`, so need to flatten them before using cross entropy:"
      ],
      "metadata": {
        "id": "1mfOFSOxEO2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(inp, targ):\n",
        "  return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ],
      "metadata": {
        "id": "qYtU_ULxELZ7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "Y1VziZP5Egzo",
        "outputId": "af4e7b4a-e93d-4de9-f564-9a854af11e44"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.329616</td>\n",
              "      <td>3.229458</td>\n",
              "      <td>0.106771</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.423797</td>\n",
              "      <td>1.894349</td>\n",
              "      <td>0.467204</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.787894</td>\n",
              "      <td>1.823074</td>\n",
              "      <td>0.466634</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.472199</td>\n",
              "      <td>1.701404</td>\n",
              "      <td>0.515299</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.240452</td>\n",
              "      <td>1.842897</td>\n",
              "      <td>0.556478</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.079337</td>\n",
              "      <td>1.897636</td>\n",
              "      <td>0.556396</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.947961</td>\n",
              "      <td>1.920025</td>\n",
              "      <td>0.565674</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.846254</td>\n",
              "      <td>1.978843</td>\n",
              "      <td>0.574137</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.779088</td>\n",
              "      <td>2.027684</td>\n",
              "      <td>0.599609</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.707870</td>\n",
              "      <td>2.146893</td>\n",
              "      <td>0.605143</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.664194</td>\n",
              "      <td>2.023275</td>\n",
              "      <td>0.611409</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.635572</td>\n",
              "      <td>2.115101</td>\n",
              "      <td>0.611735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.601438</td>\n",
              "      <td>2.120221</td>\n",
              "      <td>0.623291</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.577467</td>\n",
              "      <td>2.143193</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.565380</td>\n",
              "      <td>2.160250</td>\n",
              "      <td>0.631266</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy is okay, but we need to train for longer, since the task has changed a bit and is more complicated now.\n",
        "\n",
        "Obvious way to bet a better model is to go deeper. Only have one linear layer between the hidden state and the output activations in our basic RNN,so maybe need to go deeper"
      ],
      "metadata": {
        "id": "adc8mu7HFll5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer RNNs\n",
        "\n",
        "In a multilayer RNN, pass the activations from one RNN into another RNN."
      ],
      "metadata": {
        "id": "-Rx0dOlfF6jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Model\n",
        "\n",
        "Can save time by using PyTorch's `RNN`class, which does what we were doing earlier, but also lets us stack multiple RNNs:"
      ],
      "metadata": {
        "id": "UXKijJdmGB0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel5(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    res, h = self.rnn(self.i_h(x), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.h_o(res)\n",
        "\n",
        "  def reset(self): self.h.zero_()"
      ],
      "metadata": {
        "id": "VfYVEevNFdwD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel5(len(vocab), 64, 2),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "TwHmBTI5HKSz",
        "outputId": "d0b7bbb1-5071-4dba-b9f8-5757a030a585"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.039893</td>\n",
              "      <td>2.633240</td>\n",
              "      <td>0.442057</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.146945</td>\n",
              "      <td>1.770291</td>\n",
              "      <td>0.468831</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.703722</td>\n",
              "      <td>1.892327</td>\n",
              "      <td>0.352702</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.489365</td>\n",
              "      <td>1.773435</td>\n",
              "      <td>0.426514</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.331773</td>\n",
              "      <td>1.703860</td>\n",
              "      <td>0.480713</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.199094</td>\n",
              "      <td>1.656880</td>\n",
              "      <td>0.521647</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.054824</td>\n",
              "      <td>1.616002</td>\n",
              "      <td>0.524333</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.934596</td>\n",
              "      <td>1.797078</td>\n",
              "      <td>0.523275</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.829899</td>\n",
              "      <td>1.858169</td>\n",
              "      <td>0.518148</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.749235</td>\n",
              "      <td>1.770244</td>\n",
              "      <td>0.549723</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.684465</td>\n",
              "      <td>1.787315</td>\n",
              "      <td>0.548258</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.639140</td>\n",
              "      <td>1.839761</td>\n",
              "      <td>0.548014</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.606669</td>\n",
              "      <td>1.843344</td>\n",
              "      <td>0.552002</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.587067</td>\n",
              "      <td>1.847261</td>\n",
              "      <td>0.552734</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.576380</td>\n",
              "      <td>1.860385</td>\n",
              "      <td>0.547445</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things got worse. What the fuck?"
      ],
      "metadata": {
        "id": "AxUyUaQnH223"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploding or Diappearing Activations\n",
        "\n",
        "Creating accurate models from this kind of RNN is difficult. Would get better results if we call `detach` less often and have more layers. But this also means we have a deeper model to train. Key challenge in DL has been to figure out how to train these kinds of models.\n",
        "\n",
        "The reason this is challenging is because of what happens when you multiply by a matrix many times. Think about what happens when you multiply by a number many times. For example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,... after 32 steps you are already at 4,294,967,296. A similar issue happens if you multiply by 0.5: you get 0.5, 0.25, 0.125… and after 32 steps it's 0.00000000023. As you can see, multiplying by a number even slightly higher or lower than 1 results in an explosion or disappearance of our starting number, after just a few repeated multiplications.\n",
        "\n",
        "Because matrix multiplication is just multiplying numbers and adding them up, exactly the same thing happens with repeated matrix multiplications. And that's all a deep neural network is —each extra layer is another matrix multiplication. This means that it is very easy for a deep neural network to end up with extremely large or extre|mely small numbers.\n",
        "\n",
        "This is a problem, because the way computers store numbers (known as \"floating point\") means that they become less and less accurate the further away the numbers get from zero. The diagram in <<float_prec>>, from the excellent article [\"What You Never Wanted to Know About Floating Point but Will Be Forced to Find Out\"](http://www.volkerschatz.com/science/float.html), shows how the precision of floating-point numbers varies over the number line.\n",
        "\n",
        "This inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks. This is commonly referred to as the *vanishing gradients* or *exploding gradients* problem. It means that in SGD, the weights are either not updated at all or jump to infinity. Either way, they won't improve with training.\n",
        "\n",
        "For RNNs, there are two types of layers that are frequently used to avoid exploding activations: *gated recurrent units* (GRUs) and *long short-term memory* (LSTM) layers. Both of these are available in PyTorch, and are drop-in replacements for the RNN layer. We will only cover LSTMs in this book; there are plenty of good tutorials online explaining GRUs, which are a minor variant on the LSTM design."
      ],
      "metadata": {
        "id": "6ZoidWpHJmUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n",
        "\n",
        "In this picture, our input $x_{t}$ enters on the left with the previous hidden state ($h_{t-1}$) and cell state ($c_{t-1}$). The four orange boxes represent four layers (our neural nets) with the activation being either sigmoid ($\\sigma$) or tanh. tanh is just a sigmoid function rescaled to the range -1 to 1. Its mathematical expression can be written like this:\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} = 2 \\sigma(2x) - 1$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function. The green circles are elementwise operations. What goes out on the right is the new hidden state ($h_{t}$) and new cell state ($c_{t}$), ready for our next input. The new hidden state is also used as output, which is why the arrow splits to go up.\n",
        "\n",
        "Let's go over the four neural nets (called *gates*) one by one and explain the diagram—but before this, notice how very little the cell state (at the top) is changed. It doesn't even go directly through a neural net! This is exactly why it will carry on a longer-term state.\n",
        "\n",
        "First, the arrows for input and old hidden state are joined together. In the RNN we wrote earlier in this chapter, we were adding them together. In the LSTM, we stack them in one big tensor. This means the dimension of our embeddings (which is the dimension of $x_{t}$) can be different than the dimension of our hidden state. If we call those `n_in` and `n_hid`, the arrow at the bottom is of size `n_in + n_hid`; thus all the neural nets (orange boxes) are linear layers with `n_in + n_hid` inputs and `n_hid` outputs.\n",
        "\n",
        "The first gate (looking from left to right) is called the *forget gate*. Since it’s a linear layer followed by a sigmoid, its output will consist of scalars between 0 and 1. We multiply this result by the cell state to determine which information to keep and which to throw away: values closer to 0 are discarded and values closer to 1 are kept. This gives the LSTM the ability to forget things about its long-term state. For instance, when crossing a period or an `xxbos` token, we would expect to it to (have learned to) reset its cell state.\n",
        "\n",
        "The second gate is called the *input gate*. It works with the third gate (which doesn't really have a name but is sometimes called the *cell gate*) to update the cell state. For instance, we may see a new gender pronoun, in which case we'll need to replace the information about gender that the forget gate removed. Similar to the forget gate, the input gate decides which elements of the cell state to update (values close to 1) or not (values close to 0). The third gate determines what those updated values are, in the range of –1 to 1 (thanks to the tanh function). The result is then added to the cell state.\n",
        "\n",
        "The last gate is the *output gate*. It determines which information from the cell state to use to generate the output. The cell state goes through a tanh before being combined with the sigmoid output from the output gate, and the result is the new hidden state.\n",
        "\n",
        "In terms of code, we can write the same steps like this:"
      ],
      "metadata": {
        "id": "YdD01ELcM03Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    self.forget_state = nn.Linear(ni + nh, nh)\n",
        "    self.input_gate = nn.Linear(ni + nh, nh)\n",
        "    self.cell_gate = n.Linear(ni + nh, nh)\n",
        "    self.output_gate = nn.Linear(ni + nh, nh)\n",
        "\n",
        "  def forward(self, input, state):\n",
        "    h, c = state\n",
        "    h = torch.cat([h, input], dim=1)\n",
        "    forget = torch.sigmoid(self.forget_gate(h))\n",
        "    c = c * forget\n",
        "    inp = torch.sigmoid(self.input_gate(h))\n",
        "    cell = torch.tanh(self.cell_gate(h))\n",
        "    c = c + inp * cell\n",
        "    out = torch.sigmoid(self.output_gate(h))\n",
        "    h = out * torch.tanh(c)\n",
        "    return h, (h, c)"
      ],
      "metadata": {
        "id": "X-14OYISHVF7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can refactor the code so that we do one giant matrix multiplication (which is better for the GPU). That takes a bit of time (need to move one of the tensors around on the GPU to have it all in an array), so use two separate layers for the input and hidden states. Code is now:"
      ],
      "metadata": {
        "id": "fLk2kBtAPzM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    self.ih = nn.Linear(ni, 4*nh)\n",
        "    self.hh = nn.Linear(nh, 4*nh)\n",
        "\n",
        "  def forward(self, input, state):\n",
        "    h, c = state\n",
        "    # Multiplying everything at once instead of 4 smaller ones\n",
        "    gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
        "    ingate, forgetgate, outgate = map(torch,sigmoid, gates[:3])\n",
        "    cellgate = gates[3].tanh()\n",
        "\n",
        "    c = (forgetgate * c) + (ingate * cellgate)\n",
        "    h = outgate * c.tanh()\n",
        "\n",
        "    return h, (h, c)"
      ],
      "metadata": {
        "id": "wu9KdNTdOv1Z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use PyTorch `chunk` to split tensor into four pieces. Example:"
      ],
      "metadata": {
        "id": "MlyqXvHak2v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.arange(0, 10); t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2648KQgk094",
        "outputId": "0e85fd20-07bc-4699-92b6-05ace6a7027a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.chunk(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnZHiLb6k8bv",
        "outputId": "a5fc0560-9ebb-4fcf-be83-3d1734865217"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Language Model Using LSTMs\n",
        "\n",
        "Same network as `LMModel5`, using a two-layer LSTM. Can train it at a higher learning rate, for a shorter time, and get a better accuracy:"
      ],
      "metadata": {
        "id": "wAhmFYjWlTNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel6(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(res)\n",
        "\n",
        "    def reset(self):\n",
        "        for h in self.h: h.zero_()\n",
        "\n"
      ],
      "metadata": {
        "id": "S0qKYvhVk9Od"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel6(len(vocab), 64, 2),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "VluxyZgjmAfI",
        "outputId": "381e9742-2df5-4cdc-b192-8252e7f58089"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.031378</td>\n",
              "      <td>2.795523</td>\n",
              "      <td>0.245036</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.242504</td>\n",
              "      <td>1.741623</td>\n",
              "      <td>0.471029</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.668135</td>\n",
              "      <td>1.780295</td>\n",
              "      <td>0.466878</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.355659</td>\n",
              "      <td>1.869319</td>\n",
              "      <td>0.513184</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.121206</td>\n",
              "      <td>2.043567</td>\n",
              "      <td>0.528239</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.938162</td>\n",
              "      <td>1.950340</td>\n",
              "      <td>0.584229</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.763232</td>\n",
              "      <td>2.022436</td>\n",
              "      <td>0.589193</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.610504</td>\n",
              "      <td>1.821868</td>\n",
              "      <td>0.618815</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.473664</td>\n",
              "      <td>1.482878</td>\n",
              "      <td>0.664388</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.369229</td>\n",
              "      <td>1.297714</td>\n",
              "      <td>0.704834</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.293893</td>\n",
              "      <td>1.315070</td>\n",
              "      <td>0.700846</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.244110</td>\n",
              "      <td>1.261199</td>\n",
              "      <td>0.707520</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.204029</td>\n",
              "      <td>1.180092</td>\n",
              "      <td>0.727214</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.178830</td>\n",
              "      <td>1.159811</td>\n",
              "      <td>0.727214</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.166383</td>\n",
              "      <td>1.168218</td>\n",
              "      <td>0.726237</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's some overfitting, so let's try regularizing things"
      ],
      "metadata": {
        "id": "C_E2pAefmdf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularizing an LSTM\n",
        "\n",
        "Can use dropout:"
      ],
      "metadata": {
        "id": "GEbSS-TsmhcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "  def __init__(self, p): self.p = p\n",
        "\n",
        "  def forward(self, x):\n",
        "    if not self.training: return x\n",
        "    mask = x.new(*x.shape).bernoulli_(1-p)\n",
        "    return x* mask.div_(1-p)"
      ],
      "metadata": {
        "id": "EEXtJkbhmLRQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`bernoulli_` method creates a tensor of random zeros (with probability `p`) and ones (prob. `1-p`), which is then multiplied by our input defore dividing by `1-p`. Note that when using `Dropout`, need to specify whether we are training or validating since it acts differently depending."
      ],
      "metadata": {
        "id": "gz_Iq1mTm8Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Regularization and Temporal Activation Regularization\n",
        "\n",
        "Both are similar to weight decay. When we use weight decay, we add a small penalty to the loss that aims at making the weights as small as possible.\n",
        "\n",
        "For activation regularization, it's the final activations from the LSTM that we try to make as small as possible, instead of the weights.\n",
        "\n",
        "First need to store the activations somewhere, then add the means of the sequares of them to the loss (with `alpha`, which is just like `wd` for weight decay)."
      ],
      "metadata": {
        "id": "wFSR-9espFxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal activation regularization is linked to the fact we are predicting tokens in a sentence. That means it's likely that the outputs of our LSTMs should somewhat make sense when we read them in order. TAR is there to encourage that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible: our activations tensor has a shape `bs x sl x n_hid`, and we read consecutive activations on the sequence length axis (the dimension in the middle). With this, TAR can be expressed as:\n",
        "\n",
        "``` python\n",
        "loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n",
        "```\n",
        "\n",
        "`alpha` and `beta` are then two hyperparameters to tune. To make this work, we need our model with dropout to return three things: the proper output, the activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout. AR is often applied on the dropped-out activations (to not penalize the activations we turned into zeros afterward) while TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps). There is then a callback called `RNNRegularizer` that will apply this regularization for us."
      ],
      "metadata": {
        "id": "zDXIO_3Sr10I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BhmZ9c6q5Op"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}